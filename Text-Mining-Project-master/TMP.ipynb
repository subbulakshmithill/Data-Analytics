{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import math\n",
    "import random\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from langdetect import detect\n",
    "from langdetect.lang_detect_exception import LangDetectException\n",
    "from nltk import RegexpTokenizer, re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from num2words import num2words\n",
    "from textblob import TextBlob\n",
    "\n",
    "csvFilePath = '/Users/subbu/Documents/GitHub/TMP/sources/Data_smaller.csv'\n",
    "negativeLabel = -1\n",
    "positiveLabel = 1\n",
    "columnOfReviewRating = 13\n",
    "columnOfReviewText = 14\n",
    "columnOfReviewTitle = 15\n",
    "reviewCounter = 0\n",
    "ENGLISH = \"en\"\n",
    "\n",
    "listOfReviews = []\n",
    "filteredReviewsList = []\n",
    "\n",
    "tokenizer = RegexpTokenizer(r\"\\w+['Â´`]?[a-zA-Z]*\")  # filter words only - but keep the hyphens\n",
    "\n",
    "\n",
    "def openCSV(path):\n",
    "    csvFile = open(path, 'r', encoding='latin1')\n",
    "    next(csvFile)  # skip first line\n",
    "    reader = csv.reader(csvFile)\n",
    "    readFile(reader)\n",
    "    csvFile.close()\n",
    "\n",
    "\n",
    "def readFile(reader):\n",
    "    global reviewCounter, listOfReviews\n",
    "\n",
    "    for line in reader:\n",
    "        # rating = line[columnOfReviewRating]  # may be useful to verify what our code found out\n",
    "        reviewText = line[columnOfReviewText]  # column 14 contains the text of the review\n",
    "        if englishLanguage(reviewText):\n",
    "            listOfReviews.insert(reviewCounter, reviewText.lower())\n",
    "            reviewCounter = reviewCounter + 1\n",
    "    listOfReviews = list(dict.fromkeys(listOfReviews))  # remove duplicates\n",
    "\n",
    "\n",
    "def englishLanguage(reviewText):\n",
    "    try:\n",
    "        language = detect(reviewText)\n",
    "    except LangDetectException:\n",
    "        return False\n",
    "\n",
    "    return language == ENGLISH\n",
    "\n",
    "\n",
    "stopWords = set(stopwords.words('english'))\n",
    "importantStopwords = {'wouldn', 'don', \"isn't\", 'nor', \"aren't\", \"couldn't\", 'needn', \"shouldn't\", 'aren', \"shan't\",\n",
    "                      \"hadn't\", 'haven', 'too', 'couldn', 'didn', \"needn't\", 'wasn', \"mustn't\", \"doesn't\", 'mightn',\n",
    "                      \"wasn't\", \"weren't\", \"haven't\", 'mustn', \"don't\", \"should've\", 'weren', \"didn't\", 'shouldn',\n",
    "                      \"won't\", 'not', 'no', 'hasn', 'ain', \"hasn't\", \"mightn't\", \"wouldn't\", 'doesn', 'hadn',\n",
    "                      'very'}  # -'but'\n",
    "stopWords = stopWords - importantStopwords\n",
    "\n",
    "\n",
    "def removeStopwordsAndPunctuations(reviews, firstInput):\n",
    "    filteredReview = []\n",
    "\n",
    "    if not firstInput:\n",
    "        reviewTokenized = tokenizer.tokenize(reviews)\n",
    "        for token in reviewTokenized:\n",
    "            if token not in stopWords:\n",
    "                filteredReview.append(token.lower())\n",
    "    else:\n",
    "        for i in range(len(reviews)):\n",
    "\n",
    "            reviewTokenized = tokenizer.tokenize(reviews[i])\n",
    "            filteredReview = []\n",
    "            for token in reviewTokenized:\n",
    "                if token not in stopWords:\n",
    "                    filteredReview.append(token.lower())\n",
    "            filteredReviewsList.append(\" \".join(filteredReview))\n",
    "    return {'review': \" \".join(filteredReview)}\n",
    "\n",
    "\n",
    "def nltkTags(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):  # JJ\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):  # VBD\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):  # NN\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):  # RB\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def lemmatizeSentence(sentence):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    nltk_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))\n",
    "    wn_tagged = map(lambda w: (w[0], nltkTags(w[1])), nltk_tagged)\n",
    "    res_words = []\n",
    "    for word, tag in wn_tagged:\n",
    "        if tag is None:\n",
    "            res_words.append(word)\n",
    "        else:\n",
    "            res_words.append(lemmatizer.lemmatize(word, tag))\n",
    "    return \" \".join(res_words)\n",
    "\n",
    "\n",
    "def convertNumtoWords(inputString, myList, index):\n",
    "    splitedData = inputString.split(' ')\n",
    "    for i in range(len(splitedData)):\n",
    "        if splitedData[i].isdigit():\n",
    "            number = int(splitedData[i])\n",
    "            numberAsWord = num2words(number)\n",
    "            splitedData[i] = str(numberAsWord)\n",
    "            str1 = \" \".join(splitedData)\n",
    "            myList[index] = str1\n",
    "\n",
    "\n",
    "def hasNumbers(inputString):\n",
    "    return any(char.isdigit() for char in inputString)\n",
    "\n",
    "\n",
    "def normalizeText():\n",
    "    global filteredReviewsList\n",
    "    for i in range(len(listOfReviews)):\n",
    "        if hasNumbers(listOfReviews[i]):\n",
    "            convertNumtoWords(listOfReviews[i], listOfReviews, i)\n",
    "    removeStopwordsAndPunctuations(listOfReviews, True)\n",
    "\n",
    "    bufList = []\n",
    "\n",
    "    for i in range(len(filteredReviewsList)):\n",
    "        for j in filteredReviewsList[i].split(\".\"):\n",
    "            bufList.append(lemmatizeSentence(j))\n",
    "\n",
    "    filteredReviewsList.clear()\n",
    "    filteredReviewsList = [bufList]\n",
    "    # print(filteredReviewsList)\n",
    "\n",
    "\n",
    "def printList(reviews):\n",
    "    print(\"---------------------------------\\n\\nReviews:\\n\")\n",
    "    for i in range(len(reviews)):\n",
    "        print(str(i + 1) + \": \" + reviews[i])\n",
    "    print(\"\\n---------------------------------\")\n",
    "\n",
    "\n",
    "def startTextMining():\n",
    "    openCSV(csvFilePath)\n",
    "    normalizeText()\n",
    "\n",
    "startTextMining()\n",
    "\n",
    "# In Training\n",
    "features = {\"service\": [\"parking\", \"pool\", \"garden\", \"wifi\", \"aircon\", \"spa\", \"gym\", \"restaurant\", \"party\", \"game\"],\n",
    "            \"cleanliness\": [\"room\", \"lobby\", \"sheets\", \"floors\", \"tables\", \"dishes\", \"restaurant\"],\n",
    "            \"location\": [\"airport\", \"bus\", \"taxi\", \"centre\", \"beach\", \"sea\", \"nature\", \"shop\", \"busy\", \"crowded\"],\n",
    "            \"accomodation\": [\"bed\", \"couch\", \"chair\", \"size\", \"comfort\", \"toilets\", \"showers\", \"noisy\", \"view\"],\n",
    "            \"food\": [\"vegan\", \"vegetarian\", \"variety\", \"health\", \"fresh\", \"hygiene\", \"buffet\", \"breakfast\", \"portion\"],\n",
    "            \"staff\": [\"friendly\", \"reliable\", \"polite\", \"helpful\", \"speed\", \"available\", \"reception\"]\n",
    "            }\n",
    "\n",
    "serviceArr = [\"parking\", \"pool\", \"garden\", \"wifi\", \"aircon\", \"spa\", \"gym\", \"restaurant\", \"party\", \"game\"]\n",
    "accomodationArr = [\"bed\", \"couch\", \"chair\", \"size\", \"comfort\", \"toilets\", \"showers\", \"noisy\", \"view\"]\n",
    "staffArr = [\"friendly\", \"reliable\", \"polite\", \"helpful\", \"speed\", \"avialable\", \"reception\"]\n",
    "cleanlinessArr = [\"room\", \"lobby\", \"sheets\", \"floors\", \"tables\", \"dishes\", \"restaurant\"]\n",
    "locationArr = [\"airport\", \"bus\", \"taxi\", \"centre\", \"beach\", \"sea\", \"nature\", \"shop\", \"busy\", \"crowded\"]\n",
    "foodArr = [\"vegan\", \"vegetarian\", \"variety\", \"health\", \"fresh\", \"hygiene\", \"buffet\", \"breakfast\", \"portion\"]\n",
    "\n",
    "featureArrays = [serviceArr, accomodationArr, staffArr, cleanlinessArr, locationArr, foodArr]\n",
    "\n",
    "labeledList = []\n",
    "allWords = []\n",
    "ourFeatureWordsAccumulated = []\n",
    "\n",
    "serviceRating = [0, 0, 0]\n",
    "accomodationRating = [0, 0, 0]\n",
    "staffRating = [0, 0, 0]\n",
    "cleanlinessRating = [0, 0, 0]\n",
    "locationRating = [0, 0, 0]\n",
    "foodRating = [0, 0, 0]\n",
    "\n",
    "ratingsOfFeatures = [serviceRating, accomodationRating, staffRating, cleanlinessRating, locationRating, foodRating]\n",
    "\n",
    "for x in range(len(listOfReviews)):\n",
    "    splitReview = re.split(r'[?!.]+', listOfReviews[x])\n",
    "    sumOfPolarities = 0\n",
    "\n",
    "    for counter in range(len(splitReview)):\n",
    "        found = False\n",
    "        currentPartOfReview = \"\"\n",
    "\n",
    "        for feature in features:\n",
    "            currentPartOfReview = splitReview[counter]\n",
    "\n",
    "            if feature in currentPartOfReview:\n",
    "                found = True\n",
    "                ourFeatureWordsAccumulated.append(feature)\n",
    "\n",
    "            for keyword in features.get(feature):\n",
    "\n",
    "                r = splitReview[counter]\n",
    "                if keyword in currentPartOfReview:\n",
    "                    found = True\n",
    "                    ourFeatureWordsAccumulated.append(keyword)\n",
    "        if found:\n",
    "            sentiment = TextBlob(currentPartOfReview).sentiment\n",
    "            filteredSentence = removeStopwordsAndPunctuations(currentPartOfReview, False)\n",
    "            tokenizedFilteredSentence = tokenizer.tokenize(filteredSentence['review'])\n",
    "\n",
    "            lastAdded = ourFeatureWordsAccumulated[len(ourFeatureWordsAccumulated) - 1]\n",
    "            n = 0\n",
    "            for array in featureArrays:\n",
    "                if lastAdded in array:\n",
    "                    ratingsOfFeatures[n] = [ratingsOfFeatures[n][0] + sentiment[0], ratingsOfFeatures[n][1] + (\n",
    "                        negativeLabel if sentiment[0] < 0 else positiveLabel), ratingsOfFeatures[n][2] + 1]\n",
    "                n = n + 1\n",
    "            labeledList.append((filteredSentence, negativeLabel if sentiment[0] < 0 else positiveLabel))\n",
    "\n",
    "            allWords.extend(tokenizedFilteredSentence)\n",
    "\n",
    "print(\"\\n\\nAccording to TextBlob our feature-categories have the following rating (accumulated -1 or +1):\\n\")\n",
    "print(\"\\nRating of Service\")\n",
    "print(ratingsOfFeatures[0][1] / (ratingsOfFeatures[0][2] if ratingsOfFeatures[0][2] > 0 else 1))\n",
    "print(\"\\nRating of Accommodation\")\n",
    "print(ratingsOfFeatures[1][1] / (ratingsOfFeatures[1][2] if ratingsOfFeatures[1][2] > 0 else 1))\n",
    "print(\"\\nRating of Staff\")\n",
    "print(ratingsOfFeatures[2][1] / (ratingsOfFeatures[2][2] if ratingsOfFeatures[2][2] > 0 else 1))\n",
    "print(\"\\nRating of Cleanliness\")\n",
    "print(ratingsOfFeatures[3][1] / (ratingsOfFeatures[3][2] if ratingsOfFeatures[3][2] > 0 else 1))\n",
    "print(\"\\nRating of Location\")\n",
    "print(ratingsOfFeatures[4][1] / (ratingsOfFeatures[4][2] if ratingsOfFeatures[4][2] > 0 else 1))\n",
    "print(\"\\nRating of Food:\")\n",
    "print(ratingsOfFeatures[5][1] / (ratingsOfFeatures[5][2] if ratingsOfFeatures[5][2] > 0 else 1))\n",
    "\n",
    "print(\"\\n\\n(accumulated floats between -1.0 and 1.0)\")\n",
    "print(\"\\nRating of Service\")\n",
    "print(ratingsOfFeatures[0][0] / (ratingsOfFeatures[0][2] if ratingsOfFeatures[0][2] > 0 else 1))\n",
    "print(\"\\nRating of Accommodation\")\n",
    "print(ratingsOfFeatures[1][0] / (ratingsOfFeatures[1][2] if ratingsOfFeatures[1][2] > 0 else 1))\n",
    "print(\"\\nRating of Staff\")\n",
    "print(ratingsOfFeatures[2][0] / (ratingsOfFeatures[2][2] if ratingsOfFeatures[2][2] > 0 else 1))\n",
    "print(\"\\nRating of Cleanliness\")\n",
    "print(ratingsOfFeatures[3][0] / (ratingsOfFeatures[3][2] if ratingsOfFeatures[3][2] > 0 else 1))\n",
    "print(\"\\nRating of Location\")\n",
    "print(ratingsOfFeatures[4][0] / (ratingsOfFeatures[4][2] if ratingsOfFeatures[4][2] > 0 else 1))\n",
    "print(\"\\nRating of Food:\")\n",
    "print(ratingsOfFeatures[5][0] / (ratingsOfFeatures[5][2] if ratingsOfFeatures[5][2] > 0 else 1))\n",
    "\n",
    "random.shuffle(labeledList)\n",
    "freqDist = nltk.FreqDist(allWords)\n",
    "wordFeatures = list(freqDist.most_common(50))[:50]\n",
    "freqDist2 = nltk.FreqDist(ourFeatureWordsAccumulated)\n",
    "\n",
    "\n",
    "def documentFeatures(sentence, test):\n",
    "    setOfSingleReview = set()\n",
    "    myFeatures = {}\n",
    "    # for word in wordFeatures:\n",
    "    for aspect in features:\n",
    "        myFeatures[aspect] = (aspect in sentence)\n",
    "        for key in features.get(aspect):\n",
    "            # myFeatures['contains({})'.format(word[0])] = (word[0] in setOfSingleReview)\n",
    "            myFeatures[key] = (key in (sentence if not test else setOfSingleReview))\n",
    "    print(myFeatures)\n",
    "    return myFeatures\n",
    "\n",
    "\n",
    "featureSets = [(documentFeatures(singleReview['review'], False), polarity) for (singleReview, polarity) in labeledList]\n",
    "\n",
    "setSize = math.ceil(len(featureSets) * 0.8)\n",
    "train_set, test_set = featureSets[:setSize], featureSets[setSize:]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "testString = \\\n",
    "    removeStopwordsAndPunctuations(\"Small but cozy room. Facilities were clean but the food was not good.\", False)[\n",
    "        'review']\n",
    "\n",
    "classifierTest = documentFeatures(testString, False)\n",
    "\n",
    "classifier.show_most_informative_features(10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
